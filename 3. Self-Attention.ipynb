{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a54ff503-f9f3-435e-8804-f3364fc3a6f3",
   "metadata": {},
   "source": [
    "<center><img src=\"picture.jpg\" width=\"600\" height=\"500\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4316445-306a-4fa7-ab1f-c54062c84d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import concatenate, Input, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D  \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K \n",
    "import os\n",
    "import shutil\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dbec20-a173-448b-aaf1-2fde8242702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define width, height, and channel variables\n",
    "width, height, channel = 64, 64, 12\n",
    "\n",
    "# Define the directory path for training data and list files in that directory\n",
    "all_files_loc_train = 'E:/Deep Course/Weeks/W11/Data/Train(without Uncertainty)/'\n",
    "all_files_train = os.listdir(all_files_loc_train)\n",
    "\n",
    "# Define the directory path for testing data and list files in that directory\n",
    "all_files_loc_test = 'E:/Deep Course/Weeks/W11/Data/Test(without Uncertainty)/'\n",
    "all_files_test = os.listdir(all_files_loc_test)\n",
    "\n",
    "# Create a dictionary to map input file names to corresponding label file names for training data\n",
    "image_label_map = {\n",
    "    \"input_file_{}.npy\".format(i + 1): \"label_file_{}.npy\".format(i + 1)\n",
    "    for i in range(int(len(all_files_train) / 2))\n",
    "}\n",
    "\n",
    "# Create a list of training data file names that contain \"input\" in their names\n",
    "partition_train = [item for item in all_files_train if \"input\" in item]\n",
    "\n",
    "# Create a dictionary to map input file names to corresponding label file names for testing data\n",
    "image_label_map_val = {\n",
    "    \"input_file_{}.npy\".format(i + 1): \"label_file_{}.npy\".format(i + 1)\n",
    "    for i in range(int(len(all_files_test) / 2))\n",
    "}\n",
    "\n",
    "# Create a list of testing data file names that contain \"input\" in their names\n",
    "partition_val = [item for item in all_files_test if \"input\" in item]\n",
    "\n",
    "# Print information about the lengths of data sets and partitions\n",
    "print('Val Len:', len(all_files_test))\n",
    "print('Len Val:', len(partition_val))\n",
    "print('Train Len:', len(all_files_train))\n",
    "print('Len Train:', len(partition_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7649555f-22fe-4873-bc6d-1bc9198f0c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom data generator class for training data\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, list_examples, batch_size=4, dim=(width, height, channel), shuffle=True):\n",
    "        # Constructor of the data generator.\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_examples = list_examples\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.list_examples) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate one batch of data\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_examples[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # This function is called at the end of each epoch.\n",
    "        self.indexes = np.arange(len(self.list_examples))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # Load individual numpy arrays and aggregate them into a batch.\n",
    "\n",
    "        X = np.empty([self.batch_size, self.dim[0], self.dim[1], self.dim[2]], dtype='float32')\n",
    "\n",
    "        # y is a one-hot encoded vector.\n",
    "        y = np.empty([self.batch_size, width, height, 1], dtype=np.float32)\n",
    "\n",
    "        # Generate data.\n",
    "\n",
    "        c = 0\n",
    "        for i in list_IDs_temp:\n",
    "\n",
    "            x_file_path = os.path.join(all_files_loc_train, i)\n",
    "            y_file_path = os.path.join(all_files_loc_train, image_label_map.get(i))\n",
    "\n",
    "            # Load sample\n",
    "            X[c, :, :, :] = np.load(x_file_path)\n",
    "            # Load labels     \n",
    "            y[c, :, :, :] = np.load(y_file_path)\n",
    "\n",
    "            c = c + 1\n",
    "\n",
    "        return X, y\n",
    "\n",
    "# Define a custom data generator class for validation data (similar structure to the training data generator)\n",
    "class ValDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, list_examples, batch_size=4, dim=(width, height, channel), shuffle=True):\n",
    "        # Constructor of the data generator.\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_examples = list_examples\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.list_examples) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate one batch of data\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_examples[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # This function is called at the end of each epoch.\n",
    "        self.indexes = np.arange(len(self.list_examples))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # Load individual numpy arrays and aggregate them into a batch.\n",
    "\n",
    "        X = np.empty([self.batch_size, self.dim[0], self.dim[1], self.dim[2]], dtype=np.float32)\n",
    "\n",
    "        # y is a one-hot encoded vector.\n",
    "        y = np.empty([self.batch_size, width, height, 1], dtype=np.float32)\n",
    "\n",
    "        # Generate data.\n",
    "\n",
    "        c = 0\n",
    "        for i in list_IDs_temp:\n",
    "\n",
    "            x_file_path = os.path.join(all_files_loc_test, i)\n",
    "            y_file_path = os.path.join(all_files_loc_test, image_label_map_val.get(i))\n",
    "\n",
    "            # Load sample\n",
    "            X[c, :, :, :] = np.load(x_file_path)\n",
    "            # Load labels     \n",
    "            y[c, :, :, :] = np.load(y_file_path)\n",
    "\n",
    "            c = c + 1\n",
    "\n",
    "        return X, y\n",
    "\n",
    "# Create instances of the custom data generators for training and validation data\n",
    "training_generator = DataGenerator(partition_train)\n",
    "validation_generator = ValDataGenerator(partition_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a02478-0622-4b30-9311-2c19c63b39e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants ALPHA and BETA\n",
    "ALPHA = 0.7\n",
    "BETA = 0.3\n",
    "\n",
    "# Define TverskyLoss function\n",
    "def TverskyLoss(targets, inputs, alpha=ALPHA, beta=BETA, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Tversky Loss function for semantic segmentation.\n",
    "    :param targets: Ground truth labels\n",
    "    :param inputs: Predicted labels\n",
    "    :param alpha: Weight for false positives\n",
    "    :param beta: Weight for false negatives\n",
    "    :param smooth: Smoothing term to prevent division by zero\n",
    "    :return: Tversky loss value\n",
    "    \"\"\"\n",
    "    # Flatten label and prediction tensors\n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "    \n",
    "    # Calculate True Positives, False Positives & False Negatives\n",
    "    TP = K.sum((inputs * targets))\n",
    "    FP = K.sum(((1 - targets) * inputs))\n",
    "    FN = K.sum((targets * (1 - inputs)))\n",
    "    \n",
    "    # Calculate Tversky score\n",
    "    Tversky = (TP + smooth) / (TP + alpha * FP + beta * FN + smooth)\n",
    "    \n",
    "    # Return Tversky loss\n",
    "    return (1 - Tversky)\n",
    "\n",
    "# Define Intersection over Union (IoU) function\n",
    "def iou(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Intersection over Union (IoU) metric for semantic segmentation.\n",
    "    :param y_true: Ground truth labels\n",
    "    :param y_pred: Predicted labels\n",
    "    :param smooth: Smoothing term to prevent division by zero\n",
    "    :return: IoU score\n",
    "    \"\"\"\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    sum_ = K.sum(y_true + y_pred)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return jac\n",
    "\n",
    "# Define Jaccard distance function\n",
    "def jac_distance(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Jaccard Distance (1 - IoU) metric for semantic segmentation.\n",
    "    :param y_true: Ground truth labels\n",
    "    :param y_pred: Predicted labels\n",
    "    :return: Jaccard distance\n",
    "    \"\"\"\n",
    "    y_truef = K.flatten(y_true)\n",
    "    y_predf = K.flatten(y_pred)\n",
    "    \n",
    "    return -iou(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f93a7c-8150-42ca-9b12-ba1dfb9243d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the self-attention mechanism as a function\n",
    "def self_attention(inputs):\n",
    "    \n",
    "    Enc=layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    \n",
    "    query=Enc\n",
    "    key=Enc\n",
    "    value=Enc\n",
    "    \n",
    "    scores =tf.matmul(query, key, transpose_b=True)\n",
    "    distribution = tf.nn.softmax(scores)\n",
    "    \n",
    "    output=tf.matmul(distribution, value)\n",
    "    \n",
    "    \n",
    "    return output\n",
    "\n",
    "# Define a custom convolutional neural network model\n",
    "def Model_(input_shape):\n",
    "    # Define the input layer with the specified input shape\n",
    "    input_ = Input(input_shape)\n",
    "\n",
    "    # First set of convolutional layers\n",
    "    x1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(.00001))(input_)\n",
    "    x1 = Conv2D(128, (3, 3), kernel_regularizer=regularizers.l2(.00001), padding='same')(x1)\n",
    "\n",
    "    # Define a series of convolutional layers with different dilation rates\n",
    "    x2 = Conv2D(64, (3, 3), dilation_rate=3, activation='relu', padding='same')(x1)\n",
    "    x3 = Conv2D(66, (3, 3), dilation_rate=6, activation='relu', padding='same')(x1)\n",
    "    x4 = Conv2D(64, (3, 3), dilation_rate=12, activation='relu', padding='same')(x1)\n",
    "    x5 = Conv2D(64, (3, 3), dilation_rate=18, activation='relu', padding='same')(x1)\n",
    "    x6 = Conv2D(64, (3, 3), dilation_rate=1, activation='relu', padding='same')(x1)\n",
    "\n",
    "    # Concatenate the output feature maps from the convolutional layers\n",
    "    x6 = concatenate([x6, x5, x2, x3, x4])\n",
    "\n",
    "    # Apply channel attention to the concatenated feature maps\n",
    "    self_out = self_attention(x6)\n",
    "    print(self_out.shape)\n",
    "\n",
    "    # Apply additional convolutional layers\n",
    "    x7 = Conv2D(32, (1, 1), padding='same')(self_out)\n",
    "    x8 = Conv2D(32, (3, 3), padding='same')(x7)\n",
    "    x9 = Conv2D(16, (3, 3), padding='same')(x8)\n",
    "\n",
    "    # Apply batch normalization\n",
    "    x10 = BatchNormalization()(x9)\n",
    "\n",
    "    # Define the output layer with sigmoid activation\n",
    "    output = Conv2D(1, (1, 1), padding='same', activation='sigmoid')(x10)\n",
    "\n",
    "    # Create and return the Keras model\n",
    "    return Model(inputs=input_, outputs=output)\n",
    "\n",
    "# Create an instance of the custom model with the specified input shape\n",
    "model = Model_((width, height, channel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e755ba-ec80-487c-a992-8d6f489c4b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the custom neural network model with the specified input shape\n",
    "model = Model_((width, height, channel))\n",
    "\n",
    "# Define the learning rate for the optimizer\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# Define the optimizer (RMSprop in this case) with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model with a custom loss function (TverskyLoss) and metrics (IoU)\n",
    "model.compile(loss=TverskyLoss, optimizer=optimizer, metrics=[iou])\n",
    "\n",
    "# Define an early stopping callback to monitor validation loss and stop training if it doesn't improve for a certain number of epochs\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=60)\n",
    "\n",
    "# Create a list of callbacks, including ModelCheckpoint to save the best model and EarlyStopping for early stopping\n",
    "callbacks = [ModelCheckpoint('E:/Deep Course/Weeks/W11/Models/SelfAttention.h5',\n",
    "                             verbose=1, save_best_only=True), early_stopping]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72097a57-3cdc-4bb1-9486-c9698410797d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history =model.fit(training_generator,epochs=500,validation_data=validation_generator,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a069ba4-50bd-4d5f-a659-95d5cf59023e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tflow",
   "language": "python",
   "name": "tflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
