{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9545315-3337-4c74-be1c-d421ab5b67c0",
   "metadata": {},
   "source": [
    "<center><img src=\"picture.jpg\" width=\"600\" height=\"500\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1243af7-9a61-4272-954d-75d34252c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import concatenate, Input, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D  \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K \n",
    "import os\n",
    "import shutil\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b06f8-28fc-4192-bf48-c71eb7e2f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define width, height, and channel variables\n",
    "width, height, channel = 64, 64, 12\n",
    "\n",
    "# Define the directory path for training data and list files in that directory\n",
    "all_files_loc_train = 'E:/Deep Course/Weeks/W11/Data/Train(without Uncertainty)/'\n",
    "all_files_train = os.listdir(all_files_loc_train)\n",
    "\n",
    "# Define the directory path for testing data and list files in that directory\n",
    "all_files_loc_test = 'E:/Deep Course/Weeks/W11/Data/Test(without Uncertainty)/'\n",
    "all_files_test = os.listdir(all_files_loc_test)\n",
    "\n",
    "# Create a dictionary to map input file names to corresponding label file names for training data\n",
    "image_label_map = {\n",
    "    \"input_file_{}.npy\".format(i + 1): \"label_file_{}.npy\".format(i + 1)\n",
    "    for i in range(int(len(all_files_train) / 2))\n",
    "}\n",
    "\n",
    "# Create a list of training data file names that contain \"input\" in their names\n",
    "partition_train = [item for item in all_files_train if \"input\" in item]\n",
    "\n",
    "# Create a dictionary to map input file names to corresponding label file names for testing data\n",
    "image_label_map_val = {\n",
    "    \"input_file_{}.npy\".format(i + 1): \"label_file_{}.npy\".format(i + 1)\n",
    "    for i in range(int(len(all_files_test) / 2))\n",
    "}\n",
    "\n",
    "# Create a list of testing data file names that contain \"input\" in their names\n",
    "partition_val = [item for item in all_files_test if \"input\" in item]\n",
    "\n",
    "# Print information about the lengths of data sets and partitions\n",
    "print('Val Len:', len(all_files_test))\n",
    "print('Len Val:', len(partition_val))\n",
    "print('Train Len:', len(all_files_train))\n",
    "print('Len Train:', len(partition_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92e9c1-d383-437a-a3ad-3c3dfc91a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom data generator class for training data\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, list_examples, batch_size=4, dim=(width, height, channel), shuffle=True):\n",
    "        # Constructor of the data generator.\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_examples = list_examples\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.list_examples) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate one batch of data\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_examples[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # This function is called at the end of each epoch.\n",
    "        self.indexes = np.arange(len(self.list_examples))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # Load individual numpy arrays and aggregate them into a batch.\n",
    "\n",
    "        X = np.empty([self.batch_size, self.dim[0], self.dim[1], self.dim[2]], dtype='float32')\n",
    "\n",
    "        # y is a one-hot encoded vector.\n",
    "        y = np.empty([self.batch_size, width, height, 1], dtype=np.float32)\n",
    "\n",
    "        # Generate data.\n",
    "\n",
    "        c = 0\n",
    "        for i in list_IDs_temp:\n",
    "\n",
    "            x_file_path = os.path.join(all_files_loc_train, i)\n",
    "            y_file_path = os.path.join(all_files_loc_train, image_label_map.get(i))\n",
    "\n",
    "            # Load sample\n",
    "            X[c, :, :, :] = np.load(x_file_path)\n",
    "            # Load labels     \n",
    "            y[c, :, :, :] = np.load(y_file_path)\n",
    "\n",
    "            c = c + 1\n",
    "\n",
    "        return X, y\n",
    "\n",
    "# Define a custom data generator class for validation data (similar structure to the training data generator)\n",
    "class ValDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, list_examples, batch_size=4, dim=(width, height, channel), shuffle=True):\n",
    "        # Constructor of the data generator.\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_examples = list_examples\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.list_examples) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate one batch of data\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_examples[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # This function is called at the end of each epoch.\n",
    "        self.indexes = np.arange(len(self.list_examples))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # Load individual numpy arrays and aggregate them into a batch.\n",
    "\n",
    "        X = np.empty([self.batch_size, self.dim[0], self.dim[1], self.dim[2]], dtype=np.float32)\n",
    "\n",
    "        # y is a one-hot encoded vector.\n",
    "        y = np.empty([self.batch_size, width, height, 1], dtype=np.float32)\n",
    "\n",
    "        # Generate data.\n",
    "\n",
    "        c = 0\n",
    "        for i in list_IDs_temp:\n",
    "\n",
    "            x_file_path = os.path.join(all_files_loc_test, i)\n",
    "            y_file_path = os.path.join(all_files_loc_test, image_label_map_val.get(i))\n",
    "\n",
    "            # Load sample\n",
    "            X[c, :, :, :] = np.load(x_file_path)\n",
    "            # Load labels     \n",
    "            y[c, :, :, :] = np.load(y_file_path)\n",
    "\n",
    "            c = c + 1\n",
    "\n",
    "        return X, y\n",
    "\n",
    "# Create instances of the custom data generators for training and validation data\n",
    "training_generator = DataGenerator(partition_train)\n",
    "validation_generator = ValDataGenerator(partition_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4b30c-7ecd-48e2-b15e-f40dcedb0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants ALPHA and BETA\n",
    "ALPHA = 0.7\n",
    "BETA = 0.3\n",
    "\n",
    "# Define TverskyLoss function\n",
    "def TverskyLoss(targets, inputs, alpha=ALPHA, beta=BETA, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Tversky Loss function for semantic segmentation.\n",
    "    :param targets: Ground truth labels\n",
    "    :param inputs: Predicted labels\n",
    "    :param alpha: Weight for false positives\n",
    "    :param beta: Weight for false negatives\n",
    "    :param smooth: Smoothing term to prevent division by zero\n",
    "    :return: Tversky loss value\n",
    "    \"\"\"\n",
    "    # Flatten label and prediction tensors\n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "    \n",
    "    # Calculate True Positives, False Positives & False Negatives\n",
    "    TP = K.sum((inputs * targets))\n",
    "    FP = K.sum(((1 - targets) * inputs))\n",
    "    FN = K.sum((targets * (1 - inputs)))\n",
    "    \n",
    "    # Calculate Tversky score\n",
    "    Tversky = (TP + smooth) / (TP + alpha * FP + beta * FN + smooth)\n",
    "    \n",
    "    # Return Tversky loss\n",
    "    return (1 - Tversky)\n",
    "\n",
    "# Define Intersection over Union (IoU) function\n",
    "def iou(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Intersection over Union (IoU) metric for semantic segmentation.\n",
    "    :param y_true: Ground truth labels\n",
    "    :param y_pred: Predicted labels\n",
    "    :param smooth: Smoothing term to prevent division by zero\n",
    "    :return: IoU score\n",
    "    \"\"\"\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    sum_ = K.sum(y_true + y_pred)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return jac\n",
    "\n",
    "# Define Jaccard distance function\n",
    "def jac_distance(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Jaccard Distance (1 - IoU) metric for semantic segmentation.\n",
    "    :param y_true: Ground truth labels\n",
    "    :param y_pred: Predicted labels\n",
    "    :return: Jaccard distance\n",
    "    \"\"\"\n",
    "    y_truef = K.flatten(y_true)\n",
    "    y_predf = K.flatten(y_pred)\n",
    "    \n",
    "    return -iou(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2bcae4-c08a-4f82-b079-74e37c1d5076",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_shape = (64, 64, 12)\n",
    "\n",
    "\n",
    "learning_rate = 0.00001\n",
    "\n",
    "batch_size = 2\n",
    "num_epochs = 500\n",
    "image_size = 64  # We'll resize input images to this size\n",
    "patch_size = 8  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 128\n",
    "num_heads = 8\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n",
    "\n",
    "\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def Ex_patches(images):\n",
    "    patch_size = 8\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=[1, patch_size, patch_size, 1],\n",
    "        strides=[1, patch_size, patch_size, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "    patch_dims = patches.shape[-1]\n",
    "    \n",
    "    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "    \n",
    "    return patches\n",
    "\n",
    "\n",
    "def patch_encoder(patch, num_patches, projection_dim):\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "    \n",
    "    emd=layers.Embedding(input_dim=num_patches, output_dim=projection_dim)(positions)\n",
    "    dens=layers.Dense(units=projection_dim)(patch)\n",
    "    \n",
    "    encoded = emd + dens\n",
    "\n",
    "    return encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3127e400-5c8f-4ab4-90a2-bcd913ff5be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    \n",
    "    # Create patches.\n",
    "    patches = Ex_patches(inputs)\n",
    "    # print(patches.shape)\n",
    "    # Encode patches.\n",
    "    encoded_patches = patch_encoder(patches, num_patches, projection_dim)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    \n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "#         print(x1.shape)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        \n",
    "#         print(attention_output.shape)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        print(x3.shape)\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # print(x3.shape)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "#         print(encoded_patches.shape)\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.1)\n",
    "\n",
    "    logits=layers.Dense(units=64*64,activation='sigmoid')(features)\n",
    "    \n",
    "    out=layers.Reshape((64,64,1))(logits)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da363374-9c8d-45ec-9a11-9c83be15081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_vit_classifier((64,64,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e9939-67df-4af5-9d8c-126e4f06ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_vit_classifier((64,64,12))\n",
    "\n",
    "# Define the learning rate for the optimizer\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# Define the optimizer (RMSprop in this case) with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model with a custom loss function (TverskyLoss) and metrics (IoU)\n",
    "model.compile(loss=TverskyLoss, optimizer=optimizer, metrics=[iou])\n",
    "\n",
    "# Define an early stopping callback to monitor validation loss and stop training if it doesn't improve for a certain number of epochs\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=60)\n",
    "\n",
    "# Create a list of callbacks, including ModelCheckpoint to save the best model and EarlyStopping for early stopping\n",
    "callbacks = [ModelCheckpoint('E:/Deep Course/Weeks/W11/Models/ViT.h5',\n",
    "                             verbose=1, save_best_only=True), early_stopping]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acb0553-6b58-4374-b74b-3b9063e6edc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history =model.fit(training_generator,epochs=500,validation_data=validation_generator,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29fda6-5932-4585-9727-115c5443f3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tflow",
   "language": "python",
   "name": "tflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
