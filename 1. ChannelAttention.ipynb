{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eeee03d-5192-4e20-bc91-14f283ffdf0f",
   "metadata": {},
   "source": [
    "<center><img src=\"picture.jpg\" width=\"600\" height=\"500\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "127ec93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import concatenate, Input, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D  \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K \n",
    "import os\n",
    "import shutil\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b82c8-29cd-4687-9376-293c4614dc36",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc30b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Len: 1370\n",
      "Len Val: 685\n",
      "Train Len: 19236\n",
      "Len Train: 9618\n"
     ]
    }
   ],
   "source": [
    "# Define width, height, and channel variables\n",
    "width, height, channel = 64, 64, 12\n",
    "\n",
    "# Define the directory path for training data and list files in that directory\n",
    "all_files_loc_train = 'E:/Deep Course/Weeks/W11/Data/Train(without Uncertainty)/'\n",
    "all_files_train = os.listdir(all_files_loc_train)\n",
    "\n",
    "# Define the directory path for testing data and list files in that directory\n",
    "all_files_loc_test = 'E:/Deep Course/Weeks/W11/Data/Test(without Uncertainty)/'\n",
    "all_files_test = os.listdir(all_files_loc_test)\n",
    "\n",
    "# Create a dictionary to map input file names to corresponding label file names for training data\n",
    "image_label_map = {\n",
    "    \"input_file_{}.npy\".format(i + 1): \"label_file_{}.npy\".format(i + 1)\n",
    "    for i in range(int(len(all_files_train) / 2))\n",
    "}\n",
    "\n",
    "# Create a list of training data file names that contain \"input\" in their names\n",
    "partition_train = [item for item in all_files_train if \"input\" in item]\n",
    "\n",
    "# Create a dictionary to map input file names to corresponding label file names for testing data\n",
    "image_label_map_val = {\n",
    "    \"input_file_{}.npy\".format(i + 1): \"label_file_{}.npy\".format(i + 1)\n",
    "    for i in range(int(len(all_files_test) / 2))\n",
    "}\n",
    "\n",
    "# Create a list of testing data file names that contain \"input\" in their names\n",
    "partition_val = [item for item in all_files_test if \"input\" in item]\n",
    "\n",
    "# Print information about the lengths of data sets and partitions\n",
    "print('Val Len:', len(all_files_test))\n",
    "print('Len Val:', len(partition_val))\n",
    "print('Train Len:', len(all_files_train))\n",
    "print('Len Train:', len(partition_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a62ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom data generator class for training data\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, list_examples, batch_size=4, dim=(width, height, channel), shuffle=True):\n",
    "        # Constructor of the data generator.\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_examples = list_examples\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.list_examples) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate one batch of data\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_examples[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # This function is called at the end of each epoch.\n",
    "        self.indexes = np.arange(len(self.list_examples))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # Load individual numpy arrays and aggregate them into a batch.\n",
    "\n",
    "        X = np.empty([self.batch_size, self.dim[0], self.dim[1], self.dim[2]], dtype='float32')\n",
    "\n",
    "        # y is a one-hot encoded vector.\n",
    "        y = np.empty([self.batch_size, width, height, 1], dtype=np.float32)\n",
    "\n",
    "        # Generate data.\n",
    "\n",
    "        c = 0\n",
    "        for i in list_IDs_temp:\n",
    "\n",
    "            x_file_path = os.path.join(all_files_loc_train, i)\n",
    "            y_file_path = os.path.join(all_files_loc_train, image_label_map.get(i))\n",
    "\n",
    "            # Load sample\n",
    "            X[c, :, :, :] = np.load(x_file_path)\n",
    "            # Load labels     \n",
    "            y[c, :, :, :] = np.load(y_file_path)\n",
    "\n",
    "            c = c + 1\n",
    "\n",
    "        return X, y\n",
    "\n",
    "# Define a custom data generator class for validation data (similar structure to the training data generator)\n",
    "class ValDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, list_examples, batch_size=4, dim=(width, height, channel), shuffle=True):\n",
    "        # Constructor of the data generator.\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_examples = list_examples\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.list_examples) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate one batch of data\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_examples[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # This function is called at the end of each epoch.\n",
    "        self.indexes = np.arange(len(self.list_examples))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # Load individual numpy arrays and aggregate them into a batch.\n",
    "\n",
    "        X = np.empty([self.batch_size, self.dim[0], self.dim[1], self.dim[2]], dtype=np.float32)\n",
    "\n",
    "        # y is a one-hot encoded vector.\n",
    "        y = np.empty([self.batch_size, width, height, 1], dtype=np.float32)\n",
    "\n",
    "        # Generate data.\n",
    "\n",
    "        c = 0\n",
    "        for i in list_IDs_temp:\n",
    "\n",
    "            x_file_path = os.path.join(all_files_loc_test, i)\n",
    "            y_file_path = os.path.join(all_files_loc_test, image_label_map_val.get(i))\n",
    "\n",
    "            # Load sample\n",
    "            X[c, :, :, :] = np.load(x_file_path)\n",
    "            # Load labels     \n",
    "            y[c, :, :, :] = np.load(y_file_path)\n",
    "\n",
    "            c = c + 1\n",
    "\n",
    "        return X, y\n",
    "\n",
    "# Create instances of the custom data generators for training and validation data\n",
    "training_generator = DataGenerator(partition_train)\n",
    "validation_generator = ValDataGenerator(partition_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b326dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants ALPHA and BETA\n",
    "ALPHA = 0.7\n",
    "BETA = 0.3\n",
    "\n",
    "# Define TverskyLoss function\n",
    "def TverskyLoss(targets, inputs, alpha=ALPHA, beta=BETA, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Tversky Loss function for semantic segmentation.\n",
    "    :param targets: Ground truth labels\n",
    "    :param inputs: Predicted labels\n",
    "    :param alpha: Weight for false positives\n",
    "    :param beta: Weight for false negatives\n",
    "    :param smooth: Smoothing term to prevent division by zero\n",
    "    :return: Tversky loss value\n",
    "    \"\"\"\n",
    "    # Flatten label and prediction tensors\n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "    \n",
    "    # Calculate True Positives, False Positives & False Negatives\n",
    "    TP = K.sum((inputs * targets))\n",
    "    FP = K.sum(((1 - targets) * inputs))\n",
    "    FN = K.sum((targets * (1 - inputs)))\n",
    "    \n",
    "    # Calculate Tversky score\n",
    "    Tversky = (TP + smooth) / (TP + alpha * FP + beta * FN + smooth)\n",
    "    \n",
    "    # Return Tversky loss\n",
    "    return (1 - Tversky)\n",
    "\n",
    "# Define Intersection over Union (IoU) function\n",
    "def iou(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Intersection over Union (IoU) metric for semantic segmentation.\n",
    "    :param y_true: Ground truth labels\n",
    "    :param y_pred: Predicted labels\n",
    "    :param smooth: Smoothing term to prevent division by zero\n",
    "    :return: IoU score\n",
    "    \"\"\"\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    sum_ = K.sum(y_true + y_pred)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return jac\n",
    "\n",
    "# Define Jaccard distance function\n",
    "def jac_distance(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Jaccard Distance (1 - IoU) metric for semantic segmentation.\n",
    "    :param y_true: Ground truth labels\n",
    "    :param y_pred: Predicted labels\n",
    "    :return: Jaccard distance\n",
    "    \"\"\"\n",
    "    y_truef = K.flatten(y_true)\n",
    "    y_predf = K.flatten(y_pred)\n",
    "    \n",
    "    return -iou(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0f8d58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 shape:  (None, 64, 64, 128)\n",
      "concatenate:  (None, 64, 64, 322)\n",
      "input Tensor shape:  (None, 64, 64, 322)\n",
      "gap: (None, 1, 1, 322)\n",
      "fc1: (None, 1, 1, 100)\n",
      "fc2: (None, 1, 1, 322)\n",
      "attention: (None, 1, 1, 322)\n",
      "scaled_input: (None, 64, 64, 322)\n",
      "channel_out: (None, 64, 64, 322)\n"
     ]
    }
   ],
   "source": [
    "# Define the channel attention block\n",
    "def channel_attention(input_tensor):\n",
    "    # Get the number of channels in the input tensor\n",
    "    channels = input_tensor.shape[-1]\n",
    "\n",
    "    # Print the shape of the input tensor\n",
    "    print('input Tensor shape: ', input_tensor.shape)\n",
    "\n",
    "    # Global average pooling to get channel-wise information\n",
    "    gap = tf.reduce_mean(input_tensor, axis=[1, 2], keepdims=True)\n",
    "    print('gap:', gap.shape)\n",
    "\n",
    "    # Fully connected layers to compute channel-wise attention weights\n",
    "    fc1 = layers.Dense(100, activation='relu')(gap)\n",
    "    print('fc1:', fc1.shape)\n",
    "    fc2 = layers.Dense(channels, activation='sigmoid')(fc1)\n",
    "    print('fc2:', fc2.shape)\n",
    "\n",
    "    # Reshape the attention weights and scale the input tensor\n",
    "    attention=Reshape((1,1,channels))(fc2)\n",
    "    \n",
    "    # attention = tf.reshape(fc2, (1, 1, 1, channels))\n",
    "    print('attention:', attention.shape)\n",
    "    scaled_input = input_tensor * attention\n",
    "    print('scaled_input:', scaled_input.shape)\n",
    "\n",
    "    return scaled_input\n",
    "\n",
    "# Define a custom convolutional neural network model\n",
    "def Model_(input_shape):\n",
    "    # Define the input layer with the specified input shape\n",
    "    input_ = Input(input_shape)\n",
    "\n",
    "    # First set of convolutional layers\n",
    "    x1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(.00001))(input_)\n",
    "    x1 = Conv2D(128, (3, 3), kernel_regularizer=regularizers.l2(.00001), padding='same')(x1)\n",
    "    print('x1 shape: ',x1.shape)\n",
    "\n",
    "    # Define a series of convolutional layers with different dilation rates\n",
    "    x2 = Conv2D(64, (3, 3), dilation_rate=3, activation='relu', padding='same')(x1)\n",
    "    x3 = Conv2D(66, (3, 3), dilation_rate=6, activation='relu', padding='same')(x1)\n",
    "    x4 = Conv2D(64, (3, 3), dilation_rate=12, activation='relu', padding='same')(x1)\n",
    "    x5 = Conv2D(64, (3, 3), dilation_rate=18, activation='relu', padding='same')(x1)\n",
    "    x6 = Conv2D(64, (3, 3), dilation_rate=1, activation='relu', padding='same')(x1)\n",
    "\n",
    "    # Concatenate the output feature maps from the convolutional layers\n",
    "    x6 = concatenate([x6, x5, x2, x3, x4])\n",
    "    print('concatenate: ',x6.shape)\n",
    "\n",
    "    # Apply channel attention to the concatenated feature maps\n",
    "    channel_out = channel_attention(x6)\n",
    "    print('channel_out:', channel_out.shape)\n",
    "\n",
    "    # Apply additional convolutional layers\n",
    "    x7 = Conv2D(32, (1, 1), padding='same')(channel_out)\n",
    "    x8 = Conv2D(32, (3, 3), padding='same')(x7)\n",
    "    x9 = Conv2D(16, (3, 3), padding='same')(x8)\n",
    "\n",
    "    # Apply batch normalization\n",
    "    x10 = BatchNormalization()(x9)\n",
    "\n",
    "    # Define the output layer with sigmoid activation\n",
    "    output = Conv2D(1, (1, 1), padding='same', activation='sigmoid')(x10)\n",
    "\n",
    "    # Create and return the Keras model\n",
    "    return Model(inputs=input_, outputs=output)\n",
    "\n",
    "# Create an instance of the custom model with the specified input shape\n",
    "model = Model_((width, height, channel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf28aae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 shape:  (None, 64, 64, 128)\n",
      "concatenate:  (None, 64, 64, 322)\n",
      "input Tensor shape:  (None, 64, 64, 322)\n",
      "gap: (None, 1, 1, 322)\n",
      "fc1: (None, 1, 1, 100)\n",
      "fc2: (None, 1, 1, 322)\n",
      "attention: (None, 1, 1, 322)\n",
      "scaled_input: (None, 64, 64, 322)\n",
      "channel_out: (None, 64, 64, 322)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the custom neural network model with the specified input shape\n",
    "model = Model_((width, height, channel))\n",
    "\n",
    "# Define the learning rate for the optimizer\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# Define the optimizer (RMSprop in this case) with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model with a custom loss function (TverskyLoss) and metrics (IoU)\n",
    "model.compile(loss=TverskyLoss, optimizer=optimizer, metrics=[iou])\n",
    "\n",
    "# Define an early stopping callback to monitor validation loss and stop training if it doesn't improve for a certain number of epochs\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=60)\n",
    "\n",
    "# Create a list of callbacks, including ModelCheckpoint to save the best model and EarlyStopping for early stopping\n",
    "callbacks = [ModelCheckpoint('E:/Deep Course/Weeks/W11/Models/channelAttention.h5',\n",
    "                             verbose=1, save_best_only=True), early_stopping]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fbe4670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2404/2404 [==============================] - 185s 74ms/step - loss: 0.7604 - iou: 0.1579 - val_loss: 0.6810 - val_iou: 0.2013\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68100, saving model to E:/Deep Course/Weeks/W11/Models\\channelAttention.h5\n",
      "Epoch 2/500\n",
      "2404/2404 [==============================] - 138s 57ms/step - loss: 0.6750 - iou: 0.2019 - val_loss: 0.6730 - val_iou: 0.2115\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.68100 to 0.67305, saving model to E:/Deep Course/Weeks/W11/Models\\channelAttention.h5\n",
      "Epoch 3/500\n",
      "2404/2404 [==============================] - 138s 57ms/step - loss: 0.6722 - iou: 0.2030 - val_loss: 0.6664 - val_iou: 0.2067\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.67305 to 0.66643, saving model to E:/Deep Course/Weeks/W11/Models\\channelAttention.h5\n",
      "Epoch 4/500\n",
      "2404/2404 [==============================] - 138s 57ms/step - loss: 0.6689 - iou: 0.2062 - val_loss: 0.6671 - val_iou: 0.2019\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.66643\n",
      "Epoch 5/500\n",
      "1463/2404 [=================>............] - ETA: 53s - loss: 0.6673 - iou: 0.2065"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1198\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1196\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1197\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1198\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1200\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tflow\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:456\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 456\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tflow\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:316\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    314\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 316\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(hook))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tflow\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:336\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    333\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 336\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    339\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tflow\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:374\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    373\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 374\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tflow\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1052\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1052\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tflow\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1124\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1120\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1123\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1124\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1125\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tflow\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:529\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m    527\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t  \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m--> 529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tflow\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tflow\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tflow\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:525\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    524\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, ops\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 525\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m    527\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1094\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \n\u001b[0;32m   1073\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1094\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1060\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1059\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1061\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m     six\u001b[38;5;241m.\u001b[39mraise_from(core\u001b[38;5;241m.\u001b[39m_status_to_exception(e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmessage), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history =model.fit(training_generator,epochs=500,validation_data=validation_generator,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b54d6-b85f-4f0d-be1a-131edc76b713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a0109a-19c2-40ad-bf87-bd9974777050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tflow",
   "language": "python",
   "name": "tflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
