{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a4c6f97-13f9-403b-8430-c02ca0bedbe6",
   "metadata": {},
   "source": [
    "<center><img src=\"picture.jpg\" width=\"600\" height=\"500\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb6ac8",
   "metadata": {},
   "source": [
    "# Vit With Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a353c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e70c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of classes in the CIFAR-100 dataset.\n",
    "num_classes = 100\n",
    "\n",
    "# Define the input shape for the images.\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# Load the CIFAR-100 dataset into training and testing sets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "# Print the shapes of the training and testing data and labels.\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Create an array 'y' filled with zeros, where each row corresponds to a training sample,\n",
    "# and the column corresponding to the class label is set to 1 for one-hot encoding.\n",
    "y = np.zeros((y_train.shape[0], num_classes), dtype='float32')\n",
    "\n",
    "for i in range(y_train.shape[0]):\n",
    "    # Set the element at the i-th row and y_train[i][0]-th column to 1 for one-hot encoding.\n",
    "    y[i, y_train[i][0]] = 1\n",
    "\n",
    "# Update 'y_train' to hold the one-hot encoded labels.\n",
    "y_train = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2fc57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a custom Vision Transformer (ViT) classifier model.\n",
    "def create_vit_classifier(input_shape):\n",
    "    # Define the input layer with the specified input shape.\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    \n",
    "    # Patch Extraction\n",
    "    patch_size = 4  # Define the patch size for patch extraction.\n",
    "    batch_size = tf.shape(inputs)[0]  # Get the batch size from the input tensor.\n",
    "    \n",
    "    \n",
    "    # Extract patches from the input images.\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=inputs,\n",
    "        sizes=[1, patch_size, patch_size, 1],\n",
    "        strides=[1, patch_size, patch_size, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    patch_dims = patches.shape[-1]  # Get the dimension of the extracted patches.\n",
    "    \n",
    "\n",
    "    # Reshape the extracted patches.\n",
    "    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "\n",
    "\n",
    "    #####################################################################\n",
    "    # Patch Encoding\n",
    "    num_patches = 64  # Define the number of patches.\n",
    "    projection_dim = 64  # Define the dimension of the projection.\n",
    "\n",
    "    # Generate positional embeddings using an embedding layer.\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "    emd = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)(positions)\n",
    "    \n",
    "\n",
    "    # Apply a dense layer to encode the patches and add positional embeddings.\n",
    "    dens = layers.Dense(units=projection_dim)(patches)\n",
    "    encoded_patches = emd + dens\n",
    "\n",
    "    ###########################################################################\n",
    "\n",
    "    ################################# Transformer Layers ###############################################################\n",
    "    projection_dim = 64  # Redefine the projection dimension.\n",
    "    transformer_units = 32  # Define the number of units in the transformer layers.\n",
    "    num_heads = 2  # Define the number of attention heads.\n",
    "    dropout_rate = 0.1  # Define the dropout rate.\n",
    "\n",
    "    ######################Layer1##############################################################\n",
    "    # Layer normalization.\n",
    "    x1_1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    # Create a multi-head attention layer.\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "    )(x1_1, x1_1)\n",
    "\n",
    "    # Skip connection 1.\n",
    "    x2_1 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "    # Layer normalization 2.\n",
    "    x3_1 = layers.LayerNormalization(epsilon=1e-6)(x2_1)\n",
    "\n",
    "    # MLP.\n",
    "    x3_1 = layers.Dense(128, activation=tf.nn.gelu)(x3_1)\n",
    "    x3_1 = layers.Dropout(dropout_rate)(x3_1)\n",
    "    x3_1 = layers.Dense(64, activation=tf.nn.gelu)(x3_1)\n",
    "    x3_1 = layers.Dropout(dropout_rate)(x3_1)\n",
    "\n",
    "    # Skip connection 2.\n",
    "    encoded_patches = layers.Add()([x3_1, x2_1])\n",
    "\n",
    "    ######################Layer2##############################################################\n",
    "    # (Similar structure as Layer1, but with different variable names)\n",
    "    \n",
    "    ######################Layer3##############################################################\n",
    "    # (Similar structure as Layer1, but with different variable names)\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "\n",
    "    # Add MLP layers for further feature processing.\n",
    "    features = layers.Dense(2048, activation=tf.nn.gelu)(representation)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(1024, activation=tf.nn.gelu)(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "\n",
    "    # Classify outputs with a dense layer and softmax activation.\n",
    "    num_classes = 100  # Define the number of classes for classification.\n",
    "    logits = layers.Dense(num_classes, activation='softmax')(features)\n",
    "\n",
    "    # Create the Keras model with specified inputs and outputs.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_vit_classifier(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a811e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape for the model.\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# Create the ViT classifier model by calling the 'create_vit_classifier' function with the specified input shape.\n",
    "model = create_vit_classifier(input_shape)\n",
    "\n",
    "# Print a summary of the model's architecture.\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0639aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size, number of epochs, and learning rate.\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# Define the optimizer (Adam optimizer) with the specified learning rate.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model with the optimizer, loss function (BinaryCrossentropy), and metrics (accuracy).\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics='accuracy'\n",
    ")\n",
    "\n",
    "# Train the model on the training data.\n",
    "history = model.fit(\n",
    "    x=x_train,                  # Training data features (input images)\n",
    "    y=y_train,                  # Training data labels (one-hot encoded)\n",
    "    batch_size=batch_size,      # Number of samples per batch\n",
    "    epochs=num_epochs,          # Number of training epochs\n",
    "    validation_split=0.1       # Fraction of training data to use for validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71f2fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494e40f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9249421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff1b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tflow",
   "language": "python",
   "name": "tflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
